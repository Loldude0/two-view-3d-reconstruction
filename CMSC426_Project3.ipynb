{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*UID*:\n"
      ],
      "metadata": {
        "id": "ShO9M1By-1jl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CMSC426 Project 3: From Pixels to 3D Worlds : Two-View 3D Reconstruction**"
      ],
      "metadata": {
        "id": "hCfjjKKhR83h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "We've spent a lot of time working with images, mostly in 2D scenes. Remember Project 1, where we stitched images together by finding common features between them? Well, now it’s time to take things up a notch! Imagine being able to create a 3D view of a scene using just two images and figuring out exactly where the camera was positioned relative to that scene. Sounds exciting, right?\n",
        "\n",
        "This approach is actually a simplified version of something called Structure from Motion (SfM). While SfM usually involves reconstructing 3D structures from many images taken from different angles, we’ll start with something more manageable: Two-View Reconstruction. Instead of dealing with a whole collection of viewpoints, we’ll be focusing on building a 3D structure using only two images taken from different perspectives. The key idea? Matching features between these two images to reconstruct the scene in 3D and understand how the camera moved between shots.\n",
        "\n",
        "* Feature Matching\n",
        "* Outlier Rejection using RANSAC & Estimating the Fundamental Matrix\n",
        "*  Estimating the Essential Matrix from the Fundamental Matrix\n",
        "*  Estimating Camera Pose from the Essential Matrix\n",
        "* Checking for Cheirality Condition using Triangulation\n",
        "* Linear Triangulation of Points\n",
        "\n",
        "Reading Module : https://cmsc733.github.io/2022/proj/p3/\n",
        "\n",
        "Video Lecture : [link](https://drive.google.com/file/d/1KxBYehdtbVpH4A0vsNwbLpuLsGNqYH8F/view?usp=sharing)"
      ],
      "metadata": {
        "id": "oYiltHdjM_V_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Allowed functions: Any functions regarding reading, writing and displaying/plotting images in cv2, matplotlib\n",
        "\n",
        "Basic math utitlies including convolution operations in numpy and math\n",
        "\n"
      ],
      "metadata": {
        "id": "lmJWzv1N4T93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Feature Matching** [5 points]"
      ],
      "metadata": {
        "id": "QbaNu-pjxEoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data from Google Drive\n",
        "import gdown\n",
        "gdown.download_folder(id=\"1ROQl0NTBnxMjLrO5qISyxcTCJUPSMsRS\", quiet=True, use_cookies=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfI1tRh4uU4C",
        "outputId": "fafd8a94-7e6b-47ef-81db-f6199bc338b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/project3/feature_points.npz',\n",
              " '/content/project3/image1.jpg',\n",
              " '/content/project3/image2.jpg',\n",
              " '/content/project3/intrinsics.npz']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_matches(img1, img2, pts1, pts2):\n",
        "    \"\"\"\n",
        "    Visualize the matching points between two images.\n",
        "\n",
        "    Input:\n",
        "        img1: Image 1 in numpy array format.\n",
        "        img2: Image 2 in numpy array format.\n",
        "        pts1: Matched points in Image 1 (numpy array of shape Nx2).\n",
        "        pts2: Matched points in Image 2 (numpy array of shape Nx2).\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "Gf3el4FMVIoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(image1_path, image2_path,npz_path):\n",
        "    # Load images\n",
        "    img1 = cv2.imread(image1_path)\n",
        "    img2 = cv2.imread(image2_path)\n",
        "\n",
        "    # Load correspondences\n",
        "    data = np.load(npz_path)\n",
        "    pts1 = data['pts1']  # Nx2 array of points in the first image\n",
        "    pts2 = data['pts2']  # Nx2 array of points in the second image\n",
        "    return img1, img2,pts1, pts2"
      ],
      "metadata": {
        "id": "W6IXq2AbuZAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# File paths\n",
        "image1_path = '/content/project3/image1.jpg'\n",
        "image2_path = '/content/project3/image2.jpg'\n",
        "npz_path = '/content/project3/feature_points.npz'\n",
        "intrinsics = np.load('/content/project3/intrinsics.npz')\n",
        "K1 = intrinsics['K1']\n",
        "K2 = intrinsics['K2']\n",
        "# Load and display matches\n",
        "img1, img2,pts1, pts2 = load_data(image1_path, image2_path,npz_path)\n",
        "\n",
        "display_matches(img1, img2, pts1, pts2)"
      ],
      "metadata": {
        "id": "dCpZegTnucgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?export=view&id=1wD6L8Zl2FM75HLn8sy9ub-13ae4Jz0Hk\">\n"
      ],
      "metadata": {
        "id": "-cllpMGFwCcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Fundamental Matrix Estimation and RANSAC** [15 points]\n",
        "<img src=\"https://cmsc733.github.io/assets/2019/p3/ransac.png\">"
      ],
      "metadata": {
        "id": "O_mXzvCPVfSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_Fmatrix(img1_pts, img2_pts):\n",
        "    \"\"\"\n",
        "    Estimate the Fundamental Matrix using matched points from two images.\n",
        "\n",
        "    Input:\n",
        "        img1_pts: Matched points from Image 1 (numpy array of shape Nx2).\n",
        "        img2_pts: Matched points from Image 2 (numpy array of shape Nx2).\n",
        "\n",
        "    Output:\n",
        "        F: Estimated Fundamental Matrix (3x3 numpy array).\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "yb367guDVejd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ransac(pts1, pts2, iterations=1000, threshold=0.05):\n",
        "    \"\"\"\n",
        "    Apply RANSAC to estimate the Fundamental Matrix robustly.\n",
        "\n",
        "    Input:\n",
        "        pts1: Points from Image 1 (numpy array of shape Nx2).\n",
        "        pts2: Points from Image 2 (numpy array of shape Nx2).\n",
        "        iterations: Number of RANSAC iterations (int).\n",
        "        threshold: Distance threshold\n",
        "\n",
        "    Output:\n",
        "        img1_inliers: Inliers from Image 1 (numpy array of shape Mx2).\n",
        "        img2_inliers: Inliers from Image 2 (numpy array of shape Mx2).\n",
        "        best_F: Best Fundamental Matrix found (3x3 numpy array).\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "TWGhnnzzVMtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#img1_points, img2_points, F = ransac(pts1, pts2)\n",
        "#print(\"Fundamental Matrix:\", F)"
      ],
      "metadata": {
        "id": "11xbo7zQxm18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Draw epipolar lines** [10 points]"
      ],
      "metadata": {
        "id": "4VDdMz1yVwhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_epipolar_lines_manual(pts, F):\n",
        "    \"\"\"\n",
        "    Compute epipolar lines for given points using the Fundamental Matrix.\n",
        "\n",
        "    Input:\n",
        "        pts: Points in homogeneous coordinates (numpy array of shape Nx2).\n",
        "        F: Fundamental Matrix (3x3 numpy array).\n",
        "\n",
        "    Output:\n",
        "        lines: Epipolar lines in homogeneous form (Nx3 numpy array).\n",
        "\n",
        "    Comments:\n",
        "        - Converts points to homogeneous coordinates.\n",
        "        - Computes lines using the Fundamental Matrix.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "# Function to draw epipolar lines on images\n",
        "def draw_epipolar_lines_img1_points_img2_lines(img1, img2, pts1, F):\n",
        "    \"\"\"\n",
        "    Draw epipolar lines on the second image for points from the first image.\n",
        "\n",
        "    Input:\n",
        "        img1: Image 1 in numpy array format.\n",
        "        img2: Image 2 in numpy array format.\n",
        "        pts1: Points in Image 1 (numpy array of shape Nx2).\n",
        "        F: Fundamental Matrix (3x3 numpy array).\n",
        "\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "ksD8k6KDVzsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw epipolar lines based on the fundamental matrix\n",
        "#draw_epipolar_lines_img1_points_img2_lines(img1, img2, img1_points, F)"
      ],
      "metadata": {
        "id": "MO7o341AxseF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?export=view&id=1dx0RYxKdiYxuajkIbJhCaVPbYuTjM7Ss\">\n"
      ],
      "metadata": {
        "id": "gTzBKaF22Cfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Estimate Essential Matrix E** [15 points]\n",
        "\n"
      ],
      "metadata": {
        "id": "jGB9ojnLWAX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_Essentialmatrix(K1, K2, F):\n",
        "    \"\"\"\n",
        "    Estimate the Essential Matrix using camera intrinsics and the Fundamental Matrix.\n",
        "\n",
        "    Input:\n",
        "        K1: Intrinsic matrix of Camera 1 (3x3 numpy array).\n",
        "        K2: Intrinsic matrix of Camera 2 (3x3 numpy array).\n",
        "        F: Fundamental Matrix (3x3 numpy array).\n",
        "\n",
        "    Output:\n",
        "        E: Estimated Essential Matrix (3x3 numpy array).\n",
        "\n",
        "    Comments:\n",
        "        - Computes the Essential Matrix as E = K2.T * F * K\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "R2R8kccNWJZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# E = estimate_Essentialmatrix(K1, K2, F)\n",
        "# print(\"Essential Matrix E:\",E )"
      ],
      "metadata": {
        "id": "I5F6VVji4BoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Extracting Poses form E matrix** [20 points]\n",
        "<img src = \"https://drive.google.com/uc?export=view&id=1vizqzdm0gB4sRWQSrvZZPjwezW2OG5tE\">\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx1jD0DEWM7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_RTset(E):\n",
        "    \"\"\"\n",
        "    Extract possible sets of rotation (R) and translation (T) from the Essential Matrix.\n",
        "\n",
        "    Input:\n",
        "        E: Essential Matrix (3x3 numpy array).\n",
        "\n",
        "    Output:\n",
        "        R: List of possible rotation matrices (4x 3x3 numpy arrays).\n",
        "        T: List of possible translation vectors (4x 3x1 numpy arrays).\n",
        "\n",
        "    Comments:\n",
        "        - Uses SVD decomposition of the Essential Matrix.\n",
        "        - Constructs four possible (R, T) pairs and ensures valid rotations"
      ],
      "metadata": {
        "id": "IMB4pY1iWijh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Linear Triangulation** [15 points]\n"
      ],
      "metadata": {
        "id": "EjF3yPgSWvO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_triangulation(R_Set, T_Set, pt1, pt2, k):\n",
        "    \"\"\"\n",
        "    Perform linear triangulation to estimate 3D points.\n",
        "\n",
        "    Input:\n",
        "        R_Set: List of possible rotation matrices (4x 3x3 numpy arrays).\n",
        "        T_Set: List of possible translation vectors (4x 3x1 numpy arrays).\n",
        "        pt1: Points in Image 1 (numpy array of shape Nx2).\n",
        "        pt2: Points in Image 2 (numpy array of shape Nx2).\n",
        "        k: Camera intrinsic matrix (3x3 numpy array).\n",
        "\n",
        "    Output:\n",
        "        points_3d_set: List of 3D points for each (R, T) pair.\n",
        "\n",
        "    Comments:\n",
        "        - Triangulates 3D points for each (R, T) pair."
      ],
      "metadata": {
        "id": "_nXTPJXOWzG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6: Cheriality condition** [20 points]"
      ],
      "metadata": {
        "id": "KfA6OPOdW9rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to select the best pose (R, T) based on the chirality condition\n",
        "def extract_pose(R_set, T_set, pts_3d_set):\n",
        "    \"\"\"\n",
        "    Select the best pose (R, T) that satisfies the chirality condition.\n",
        "\n",
        "    Input:\n",
        "        R_set: List of possible rotation matrices (4x 3x3 numpy arrays).\n",
        "        T_set: List of possible translation vectors (4x 3x1 numpy arrays).\n",
        "        pts_3d_set: List of 3D point sets for each (R, T) pair.\n",
        "\n",
        "    Output:\n",
        "        R_best: Best rotation matrix (3x3 numpy array).\n",
        "        T_best: Best translation vector (3x1 numpy array).\n",
        "        X_best: Best 3D points (Nx3 numpy array).\n",
        "        index: Index of the best (R, T) pair.\n",
        "\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "bxQZNaFHXA7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cheriality(pt, r3, t):\n",
        "    \"\"\"\n",
        "    Compute the chirality condition to determine if points are in front of the camera.\n",
        "\n",
        "    Input:\n",
        "        pt: 3D points (Nx3 numpy array).\n",
        "        r3: Third row of the rotation matrix (1x3 numpy array).\n",
        "        t: Translation vector (3x1 numpy array).\n",
        "\n",
        "    Output:\n",
        "        count_depth: Number of points with positive depth.\n",
        "\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "HU4DMdEKXPUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear Triangulation\n",
        "#point3D_set = linear_triangulation(R_set,T_set,img1_points,img2_points,K1)\n",
        "\n",
        "#TO-DO: Plot all poses with 3D points as shown in figure\n",
        "\n"
      ],
      "metadata": {
        "id": "XJGOsKFv5sUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?export=view&id=186Z0ei48e8SV-lEajldKhtilU4r_vdQI\">\n"
      ],
      "metadata": {
        "id": "VkWAp-eOFB5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Get pose of camera using chirality condition\n",
        "#R_best, T_best,X_ ,index= extract_pose(R_set,T_set,point3D_set)\n",
        "\n",
        "\n",
        "#TO-DO: Plot 3D point cloud with RGB color as shown in figure"
      ],
      "metadata": {
        "id": "7k7ms2J0FNjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?export=view&id=1KH5L6sUD0d291MAt9OZ2zHecIinTUoXR\">\n"
      ],
      "metadata": {
        "id": "hJdteKWGFPHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extra Credit: Reconstruct Your Own Scene! (30 Points)**\n",
        "\n",
        "For this part, you will run an off-the-shelf incremental SfM toolbox such as [COLMAP](https://github.com/colmap/pycolmap) on your own captured multi-view images. Please submit a GIF of the reconstructed 3D structure and the location of the cameras.\n",
        "\n",
        "For this reconstruction, you can choose your own data. This data could either be a sequence with rigid objects (e.g., a mug or a vase from your vicinity) or any scene you wish to reconstruct in 3D.\n",
        "\n",
        "\n",
        "Submission:\n",
        "\n",
        "A gif to visualize the reconstruction of the scene and location of cameras.\n",
        "<img src = \"https://drive.google.com/uc?export=view&id=1MmeeYX77xcQlDVzsDRWyusD8XMO2AY-m\">"
      ],
      "metadata": {
        "id": "nuUet9343weN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report\n",
        "You will be graded primarily based on your report.\n",
        "A demonstration of understanding of the concepts involved in the project are required show the output produced by your code.\n",
        "\n",
        "Include visualizations of the output of each stage in your pipeline (as shown in the system diagram on page 2), and a description of what you did for each step. Assume that we’re familiar with the project, so you don’t need to spend time repeating what’s already in the course notes. Instead, focus on any interesting problems you encountered and/or solutions you implemented."
      ],
      "metadata": {
        "id": "YbLVmVStFuFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission Guidelines\n",
        "\n",
        "**If your submission does not comply with the following guidelines, you’ll be given ZERO credit.**"
      ],
      "metadata": {
        "id": "OkqHKTebLX6g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uEerqCgCWglw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your submission on ELMS(Canvas) must be a pdf file, following the naming convention **YourDirectoryID_proj3.pdf**. For example, xyz123_proj3.pdf.\n",
        "\n",
        "**All your results and report should be included in this notebook. After you finished all, please export the notebook as a pdf file and submit it to ELMS(Canvas).**"
      ],
      "metadata": {
        "id": "g-Gcl_0OLmPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collaboration Policy\n",
        "You are encouraged to discuss the ideas with your peers. However, the code should be your own, and should be the result of you exercising your own understanding of it. If you reference anyone else’s code in writing your project, you must properly cite it in your code (in comments) and your writeup. For the full honor code refer to the CMSC426 Fall 2023 website."
      ],
      "metadata": {
        "id": "HRz1NLl59LOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Inspiration Credit to CMSC733 COmputer Vision and 16-822: Geometry-based Methods in Vision\n"
      ],
      "metadata": {
        "id": "Fzm58nx_4EVi"
      }
    }
  ]
}